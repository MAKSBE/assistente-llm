# ğŸ§  Projeto: Assistente de IA Local com LLM + Docker + Python

## ğŸ“Œ VisÃ£o Geral

Este projeto implementa um assistente de IA baseado em **Large Language Models (LLM)**, rodando 100% localmente, com orquestraÃ§Ã£o via **Docker**.

Ele inclui:

- Backend em Python (FastAPI)
- Frontend web simples
- IngestÃ£o e embedding de documentos
- IntegraÃ§Ã£o com modelos como **Mistral** e **CodeLlama** via **Ollama**

O objetivo Ã© fornecer uma soluÃ§Ã£o privada e customizÃ¡vel para consultas e manipulaÃ§Ã£o de dados com IA generativa.

---

## ğŸ—‚ï¸ Estrutura do Projeto

```
llm/
  backend/         # API backend em Python (FastAPI)
  frontend/        # Interface web HTML/JS
  ingest/          # Scripts de ingestÃ£o e embedding de documentos
  ollama/          # ConfiguraÃ§Ã£o do container com LLM
  docker-compose.yml
  docs/            # DocumentaÃ§Ã£o e boas prÃ¡ticas
```

---

## âš™ï¸ Principais Componentes

### ğŸ”¹ `backend/`
API FastAPI que expÃµe endpoints para consultas ao LLM e serve o frontend.

### ğŸ”¹ `frontend/`
Interface web simples e responsiva (tema escuro), construÃ­da em HTML e JS puro.

### ğŸ”¹ `ingest/`
Scripts em Python usando LangChain e ChromaDB para gerar embeddings a partir de documentos da pasta `docs/`.

### ğŸ”¹ `ollama/`
ConfiguraÃ§Ã£o Docker para execuÃ§Ã£o local de modelos via Ollama, como **Mistral** e **CodeLlama**.

### ğŸ”¹ `docker-compose.yml`
Orquestra todos os serviÃ§os automaticamente com um Ãºnico comando.

---

## ğŸš€ Funcionalidades

- Consulta a LLMs locais via API REST
- Enriquecimento do contexto com RAG (Retrieval-Augmented Generation)
- Interface web para interaÃ§Ã£o
- ExecuÃ§Ã£o local e offline com Docker

---

## ğŸ§ª Como Rodar o Projeto

### âœ… PrÃ©-requisitos

- Docker e Docker Compose instalados  
- (Opcional) Python 3.8+ para rodar scripts manualmente

### ğŸ“¦ Passos para execuÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone https://github.com/MAKSBE/assistente-llm.git
cd assistente-llm

# (Opcional) Configure variÃ¡veis de ambiente
export OLLAMA_BASE_URL=http://localhost:11434
export MODEL_NAME=mistral

# Suba os containers
docker-compose up --build
```

### ğŸŒ Acesse:

- Interface Web: [http://localhost:8000/frontend/index.html](http://localhost:8000/frontend/index.html)  
- Swagger da API: [http://localhost:8000/docs](http://localhost:8000/docs)

### ğŸ’¬ Exemplo de uso via `curl`

```bash
curl -X POST http://localhost:8000/api/query \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explique o que Ã© LLM."}'
```

---

## ğŸ§¾ Estrutura de Arquivos Relevantes

- `backend/main.py`: InicializaÃ§Ã£o da API e rotas
- `backend/mistral_client.py`: Cliente para comunicaÃ§Ã£o com Ollama
- `ingest/embed_docs.py`: GeraÃ§Ã£o de embeddings com LangChain + ChromaDB
- `docker-compose.yml`: OrquestraÃ§Ã£o dos serviÃ§os

---

## ğŸ› ï¸ Boas PrÃ¡ticas

- SeparaÃ§Ã£o clara de responsabilidades (camadas Domain, Application, Infra, API)
- Uso de DTOs (nunca exponha entidades diretamente)
- InjeÃ§Ã£o de dependÃªncia e testes facilitados
- ValidaÃ§Ãµes centralizadas
- Prompts otimizados e configurÃ¡veis
- DocumentaÃ§Ã£o na pasta `docs/`

---

## ğŸ“ˆ Melhorias Planejadas

### ğŸ”§ OtimizaÃ§Ã£o de Recursos
- Ajustar containers para rodar em mÃ¡quinas pessoais
- Permitir troca rÃ¡pida de modelos via `.env`

### ğŸ“Š Interface de AdministraÃ§Ã£o
- Painel web para status, logs, memÃ³ria, CPU e restart de serviÃ§os

### âš¡ InstalaÃ§Ã£o Facilitada
- Script para instalar Docker e configurar ambiente automaticamente
- Requisitos mÃ­nimos de hardware documentados

### ğŸ’¾ Backup e RestauraÃ§Ã£o
- ExportaÃ§Ã£o/importaÃ§Ã£o de embeddings e documentos
- Salvamento das configuraÃ§Ãµes locais

### ğŸ” SeguranÃ§a
- AutenticaÃ§Ã£o com senha Ãºnica
- Logs de acesso para auditoria

### ğŸ” AtualizaÃ§Ã£o Simplificada
- Script para atualizar cÃ³digo e modelos sem perda de dados

### ğŸ›ï¸ CustomizaÃ§Ã£o
- Permitir ajuste de prompts, contexto e temperatura do modelo via interface

### ğŸš€ Performance
- Cache local para perguntas frequentes
- Controle do tamanho de chunk/contexto

---
